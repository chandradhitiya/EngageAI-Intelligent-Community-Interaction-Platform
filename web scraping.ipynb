{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#farm made srcaping \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='scraper_errors.log', level=logging.ERROR)\n",
    "\n",
    "\n",
    "def create_robust_session():\n",
    "    \"\"\"Create a session with retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,  # number of retries\n",
    "        backoff_factor=1,  # wait 1, 2, 4, 8, 16 seconds between retries\n",
    "        status_forcelist=[500, 502, 503, 504]  # retry on these status codes\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "def extract_website_text(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "\n",
    "        print(f\"Starting extraction from: {url}\")\n",
    "        session = create_robust_session()\n",
    "\n",
    "        # Make request with increased timeout and chunked response\n",
    "        response = session.get(url, headers=headers, timeout=60, stream=True)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        content = ''\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                content += chunk.decode('utf-8', errors='ignore')\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        logging.error(f\"Request error: {e} for URL: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        logging.error(f\"Unexpected error: {e} for URL: {url}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if 'session' in locals():\n",
    "            session.close()\n",
    "\n",
    "\n",
    "def extract_product_details(soup):\n",
    "    \"\"\"Extracts product details from a product page.\"\"\"\n",
    "    details = {}\n",
    "    try:\n",
    "        details['name'] = soup.find('h1', class_='product-single__title').text.strip()  # Example\n",
    "    except AttributeError:  # Handle cases where the element is not found\n",
    "        details['name'] = None\n",
    "    try:\n",
    "        details['price'] = soup.find('span', class_='money').text.strip()  # Example\n",
    "    except AttributeError:\n",
    "        details['price'] = None\n",
    "    try:\n",
    "        details['description'] = soup.find('div', class_='product-single__description').text.strip()  # Example\n",
    "    except AttributeError:\n",
    "        details['description'] = None\n",
    "    # Extract images, variants, etc.\n",
    "    return details\n",
    "\n",
    "\n",
    "def extract_about_us(soup):\n",
    "    \"\"\"Extracts 'about us' information from the about page.\"\"\"\n",
    "    about_info = {}\n",
    "    try:\n",
    "        about_info['content'] = soup.find('div', class_='rte').text.strip()  # Example\n",
    "    except AttributeError:\n",
    "        about_info['content'] = None\n",
    "        print(\"Could not extract about us content\")\n",
    "        \n",
    "    try:\n",
    "        about_info['title'] = soup.find('h1', class_='page-title').text.strip()\n",
    "        print(f\"Extracted about us title: {about_info['title']}\")\n",
    "    except AttributeError:\n",
    "        about_info['title'] = None\n",
    "        print(\"Could not extract about us title\")\n",
    "        \n",
    "    return about_info\n",
    "\n",
    "\n",
    "def clean_extracted_data(data):\n",
    "    \"\"\"Clean and format the extracted data\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, str):\n",
    "            cleaned[key] = value.strip()\n",
    "        elif isinstance(value, list):\n",
    "            cleaned[key] = [item.strip() for item in value if item.strip()]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def save_extracted_data(data, filename='extracted_data.json'):\n",
    "    \"\"\"Save the extracted data to a JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, indent=4, ensure_ascii=False, fp=f)\n",
    "        print(f\"\\nData saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {e}\")\n",
    "        logging.error(f\"Error saving data: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    urls = {\n",
    "        'free_range_eggs': 'https://farmmadefoods.com/collections/free-range-eggs?customer_posted=true',\n",
    "        'product_page_1': 'https://farmmadefoods.com/products/farm-made-foods-free-range-eggs-6',  # Example product url Replace with a real product page\n",
    "        'product_page_2': 'https://farmmadefoods.com/products/natural-coconut-sugar',  # Replace with a real product page\n",
    "        'product_page_3': 'https://farmmadefoods.com/products/coconut-sugar-delights',  # Replace with a real product page\n",
    "        'blog_page_1': 'https://farmmadefoods.com/blogs/recipe/sunny-side-up-eggs',  # Replace with a real product page\n",
    "        'blog_page_2': 'https://farmmadefoods.com/blogs/recipe/mixed-herbs-omelette-easy-healthy-breakfast-recipe',  # Replace with a real product page\n",
    "        'blog_page_3': 'https://farmmadefoods.com/blogs/recipe/english-breakfast-recipe-easy-guide',  # Replace with a real product page\n",
    "        'blog_page_4': 'https://farmmadefoods.com/blogs/recipe/eggcellent-zesty-delight',  # Replace with a real product page\n",
    "        'blog_page_5': 'https://farmmadefoods.com/blogs/recipe/scrambled-eggs-recipe',  # Replace with a real product page\n",
    "        'contact_page_1': 'https://farmmadefoods.com/pages/contact',  # Replace with a real product page\n",
    "        'about': 'https://farmmadefoods.com/pages/about-us',  # Replace with a real product page\n",
    "        'return_policy': 'https://farmmadefoods.com/pages/refund-policy'  # Replace with the actual about us page URL\n",
    "    }\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for key, url in urls.items():\n",
    "        soup = extract_website_text(url)\n",
    "        if soup:\n",
    "            if key == 'free_range_eggs':\n",
    "                all_data['free_range_eggs'] = {'title': soup.find('title').text.strip()}  # Example extract\n",
    "            elif key == 'about':\n",
    "                all_data['about'] = extract_about_us(soup)\n",
    "            elif key.startswith('product_page'):\n",
    "                all_data[key] = extract_product_details(soup)\n",
    "            #else:\n",
    "            #    all_data[key] = {'title': soup.find('title').text.strip()} # General title extraction\n",
    "        else:\n",
    "            print(f\"Failed to extract data from {url}\")\n",
    "\n",
    "    # Save all data to a single JSON file\n",
    "    with open('combined_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, indent=4, ensure_ascii=False, fp=f)\n",
    "    print(\"Data saved to combined_data.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # url = \"https://farmmadefoods.com/collections/free-range-eggs?customer_posted=true#FooterNewsletter\"  # Example URL\n",
    "    # print(\"Starting extraction...\")\n",
    "    #\n",
    "    # start_time = time.time()\n",
    "    # extracted_content = extract_website_text(url)\n",
    "    # end_time = time.time()\n",
    "    #\n",
    "    # if extracted_content:\n",
    "    #     print(f\"\\nExtraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    #     print(\"\\nExtracted Content Summary:\")\n",
    "    #     print(\"-\" * 50)\n",
    "    #\n",
    "    #     for key, value in extracted_content.items():\n",
    "    #         if isinstance(value, str):\n",
    "    #             print(f\"\\n{key.upper()}:\")\n",
    "    #             print(value[:200] + \"...\" if len(value) > 200 else value)\n",
    "    #         elif isinstance(value, list):\n",
    "    #             print(f\"\\n{key.upper()} ({len(value)} items):\")\n",
    "    #             for item in value[:3]:  # Show first 3 items\n",
    "    #                 print(f\"- {item[:100]}...\" if len(item) > 100 else f\"- {item}\")\n",
    "    #             if len(value) > 3:\n",
    "    #                 print(f\"... and {len(value)-3} more items\")\n",
    "    #\n",
    "    #     # Save the data\n",
    "    #     save_extracted_data(extracted_content)\n",
    "    # else:\n",
    "    #     print(\"No content was extracted\")\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data for keyword 'egg prices' in r/eggs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m\n\u001b[0;32m     34\u001b[0m post_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubreddit\u001b[39m\u001b[38;5;124m'\u001b[39m: subreddit_name,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m'\u001b[39m: keyword,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m     47\u001b[0m }\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Get relevant comments discussing prices, supply, demand etc.\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[38;5;241m.\u001b[39mreplace_more(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m submission\u001b[38;5;241m.\u001b[39mcomments\u001b[38;5;241m.\u001b[39mlist():\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(kw \u001b[38;5;129;01min\u001b[39;00m comment\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m \n\u001b[0;32m     53\u001b[0m           [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarket\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupply\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortage\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\praw\\models\\reddit\\base.py:38\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[0;32m     40\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\praw\\models\\reddit\\submission.py:726\u001b[0m, in \u001b[0;36mSubmission._fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 726\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m     submission_listing, comment_listing \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    728\u001b[0m     comment_listing \u001b[38;5;241m=\u001b[39m Listing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reddit, _data\u001b[38;5;241m=\u001b[39mcomment_listing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\praw\\models\\reddit\\submission.py:744\u001b[0m, in \u001b[0;36mSubmission._fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_fetch_params\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    743\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[1;32m--> 744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\praw\\util\\deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     40\u001b[0m     warn(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\praw\\reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m retry_strategy_state\u001b[38;5;241m.\u001b[39msleep()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m do_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munauthorized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m             response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\prawcore\\rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay()\n\u001b[0;32m     46\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m set_header_callback()\n\u001b[1;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m request_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(response\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\prawcore\\requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, timeout\u001b[38;5;241m=\u001b[39mtimeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#reddit\n",
    "#market analysis\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize Reddit API connection\n",
    "user_agent = \"market_research_bot 1.0\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id='E7lnxu7KkXI9HWF0cWOFww',\n",
    "    client_secret='Ye__fRBy5a8753p62oIF9jdEt3j9Yw', \n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Define search parameters\n",
    "subreddits = [\"eggs\", \"farming\", \"agriculture\", \"food\", \"business\"]\n",
    "keywords = [\n",
    "    \"egg prices\", \"egg market\", \"egg industry\", \"egg production\",\n",
    "    \"egg supply\", \"egg demand\", \"egg shortage\", \"egg surplus\",\n",
    "    \"egg farm\", \"egg producer\", \"egg wholesale\", \"egg retail\"\n",
    "]\n",
    "limit = 200  # Posts per keyword\n",
    "\n",
    "market_data = []\n",
    "\n",
    "# Search across multiple subreddits\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Search for market-related posts\n",
    "            for submission in subreddit.search(keyword, limit=limit):\n",
    "                post_data = {\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'keyword': keyword,\n",
    "                    'title': submission.title,\n",
    "                    'content': submission.selftext,\n",
    "                    'url': submission.url,\n",
    "                    'author': str(submission.author),\n",
    "                    'score': submission.score,\n",
    "                    'upvote_ratio': submission.upvote_ratio,\n",
    "                    'num_comments': submission.num_comments,\n",
    "                    'created': time.strftime('%Y-%m-%d %H:%M:%S', \n",
    "                                          time.localtime(submission.created_utc)),\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                # Get relevant comments discussing prices, supply, demand etc.\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                for comment in submission.comments.list():\n",
    "                    if any(kw in comment.body.lower() for kw in \n",
    "                          ['price', 'cost', 'market', 'supply', 'demand', 'shortage']):\n",
    "                        comment_data = {\n",
    "                            'author': str(comment.author),\n",
    "                            'text': comment.body,\n",
    "                            'score': comment.score,\n",
    "                            'created': time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "                                                   time.localtime(comment.created_utc))\n",
    "                        }\n",
    "                        post_data['comments'].append(comment_data)\n",
    "\n",
    "                market_data.append(post_data)\n",
    "            \n",
    "            print(f\"Collected data for keyword '{keyword}' in r/{subreddit_name}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting data from r/{subreddit_name} for '{keyword}': {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save market research data\n",
    "with open('egg_market_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(market_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nCollected {len(market_data)} relevant posts\")\n",
    "print(\"Market research data saved to egg_market_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to reddit_eggs_data.json\n"
     ]
    }
   ],
   "source": [
    "#comment \n",
    "#reddit\n",
    "import praw\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='reddit_scraper.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_reddit(subreddit_name, keywords, limit=100):\n",
    "    \"\"\"\n",
    "    Scrapes Reddit for posts and comments related to the specified keywords.\n",
    "    Args:\n",
    "        subreddit_name (str): The name of the subreddit to scrape.\n",
    "        keywords (list): A list of keywords to search for.\n",
    "        limit (int): The maximum number of posts to retrieve.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing data from a relevant post.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Authenticate with Reddit API.  Replace with your credentials\n",
    "        user_agent = \"scapper 1.0 by/u/Business-Till-1699\"\n",
    "        reddit = praw.Reddit(\n",
    "            client_id='E7lnxu7KkXI9HWF0cWOFww', \n",
    "            client_secret='Ye__fRBy5a8753p62oIF9jdEt3j9Yw',\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        relevant_posts = []\n",
    "\n",
    "        for submission in subreddit.search(query=\" OR \".join(keywords), sort=\"relevance\", limit=limit):\n",
    "            post_data = {\n",
    "                'title': submission.title,\n",
    "                'url': submission.url,\n",
    "                'author': str(submission.author),\n",
    "                'subreddit': subreddit_name,\n",
    "                'upvote_ratio': submission.upvote_ratio,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'score': submission.score,\n",
    "                'created': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(submission.created)),\n",
    "                'keywords': keywords,\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            # Extract comments\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = {\n",
    "                    'author': str(comment.author),\n",
    "                    'body': comment.body,\n",
    "                    'score': comment.score,\n",
    "                    'created': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(comment.created))\n",
    "                }\n",
    "                post_data['comments'].append(comment_data)\n",
    "\n",
    "            relevant_posts.append(post_data)\n",
    "            logging.info(f\"Extracted post: {submission.title}\")\n",
    "\n",
    "        return relevant_posts\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during scraping: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    # Define subreddit and keywords\n",
    "    subreddit_name = \"eggs\"  # Adjust as needed\n",
    "    keywords = [\"eggs\", \"egg market\", \"news on eggs\", \"news on farm made company\", \"eggs market value\",\n",
    "                \"eggs consumer market\", \"eggs requirement\"]\n",
    "    limit = 100  # Number of posts to extract\n",
    "\n",
    "    # Scrape Reddit\n",
    "    reddit_data = scrape_reddit(subreddit_name, keywords, limit)\n",
    "\n",
    "    # Save data to JSON file\n",
    "    try:\n",
    "        with open(\"reddit_eggs_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(reddit_data, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Successfully saved data to reddit_eggs_data.json\")\n",
    "        logging.info(\"Successfully saved data to reddit_eggs_data.json\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl (15.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 796.8 kB/s eta 0:00:16\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     --- ----------------------------------- 1.0/12.8 MB 986.7 kB/s eta 0:00:12\n",
      "     --- ----------------------------------- 1.0/12.8 MB 986.7 kB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------- ------------------------------- 2.4/12.8 MB 823.4 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 834.0 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 834.0 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 834.0 kB/s eta 0:00:13\n",
      "     -------- ------------------------------ 2.9/12.8 MB 762.5 kB/s eta 0:00:14\n",
      "     --------- ----------------------------- 3.1/12.8 MB 785.3 kB/s eta 0:00:13\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 805.2 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.7/12.8 MB 826.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.7/12.8 MB 826.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.7/12.8 MB 826.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 767.5 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 767.5 kB/s eta 0:00:12\n",
      "     ------------ -------------------------- 4.2/12.8 MB 783.9 kB/s eta 0:00:11\n",
      "     ------------- ------------------------- 4.5/12.8 MB 789.4 kB/s eta 0:00:11\n",
      "     -------------- ------------------------ 4.7/12.8 MB 798.8 kB/s eta 0:00:11\n",
      "     --------------- ----------------------- 5.0/12.8 MB 811.7 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.2/12.8 MB 823.7 kB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 843.0 kB/s eta 0:00:09\n",
      "     ----------------- --------------------- 5.8/12.8 MB 857.2 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 868.4 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.3/12.8 MB 878.9 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 892.8 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 900.0 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.1/12.8 MB 910.6 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 926.3 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.6/12.8 MB 939.5 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.9/12.8 MB 954.0 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.9/12.8 MB 954.0 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 933.8 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 933.8 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 932.1 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 943.2 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.2/12.8 MB 950.7 kB/s eta 0:00:04\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 956.3 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.7/12.8 MB 963.3 kB/s eta 0:00:04\n",
      "     ----------------------------- -------- 10.0/12.8 MB 969.9 kB/s eta 0:00:03\n",
      "     ------------------------------ ------- 10.2/12.8 MB 979.3 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.5/12.8 MB 986.8 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 994.2 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 11.0/12.8 MB 996.8 kB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (2.2.3)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Downloading plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bertopic) (4.67.1)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.45.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.61.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.11.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
      "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
      "Downloading hdbscan-0.8.40-cp310-cp310-win_amd64.whl (730 kB)\n",
      "   ---------------------------------------- 0.0/730.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/730.9 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 524.3/730.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 730.9/730.9 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.8 MB 1.3 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/14.8 MB 859.5 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.8/14.8 MB 859.5 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.8/14.8 MB 859.5 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 1.0/14.8 MB 728.9 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.3/14.8 MB 780.2 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.8/14.8 MB 976.9 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.0 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 2.6/14.8 MB 1.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.6/14.8 MB 1.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.6/14.8 MB 1.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.9/14.8 MB 1.0 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 3.1/14.8 MB 1.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 3.4/14.8 MB 1.0 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.7/14.8 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.9/14.8 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 4.5/14.8 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 4.7/14.8 MB 1.2 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 5.2/14.8 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.5/14.8 MB 1.2 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 5.8/14.8 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 6.3/14.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 6.6/14.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.8/14.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.8/14.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.8/14.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 7.3/14.8 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 7.6/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 7.9/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 8.1/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 8.4/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 8.4/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 8.7/14.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 9.2/14.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 9.4/14.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 9.7/14.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 9.7/14.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 10.2/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 10.5/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 10.7/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 10.7/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 10.7/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 11.8/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.1/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.1/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.6/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.8/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.1/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.1/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.4/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.4/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 13.9/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/14.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading numba-0.61.0-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.3/2.8 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.6/2.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading llvmlite-0.44.0-cp310-cp310-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/30.3 MB 3.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.0/30.3 MB 3.1 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 1.3/30.3 MB 2.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/30.3 MB 2.1 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 2.4/30.3 MB 2.3 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 2.6/30.3 MB 2.3 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 3.1/30.3 MB 2.3 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 3.4/30.3 MB 2.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 3.9/30.3 MB 2.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 4.5/30.3 MB 2.2 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.0/30.3 MB 2.2 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.2/30.3 MB 2.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 5.8/30.3 MB 2.2 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 6.3/30.3 MB 2.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 6.8/30.3 MB 2.3 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 7.3/30.3 MB 2.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 7.9/30.3 MB 2.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 8.1/30.3 MB 2.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 8.7/30.3 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 9.2/30.3 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 9.4/30.3 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 10.0/30.3 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 10.0/30.3 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 10.2/30.3 MB 2.1 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 10.2/30.3 MB 2.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 10.7/30.3 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 11.0/30.3 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 11.5/30.3 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 11.8/30.3 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 12.1/30.3 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 12.6/30.3 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 12.8/30.3 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 13.1/30.3 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 13.4/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 13.4/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 13.4/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 13.4/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 14.7/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 15.2/30.3 MB 1.9 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 15.7/30.3 MB 1.9 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 16.3/30.3 MB 1.9 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 16.5/30.3 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 16.8/30.3 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 17.3/30.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 17.6/30.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 18.1/30.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 18.4/30.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 18.9/30.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 19.7/30.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 19.9/30.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 20.2/30.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 20.2/30.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 20.2/30.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 20.4/30.3 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 20.7/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 21.0/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 21.5/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 21.5/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 21.8/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 21.8/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 22.0/30.3 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 22.3/30.3 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 22.5/30.3 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 22.8/30.3 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 23.3/30.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 23.6/30.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 24.1/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.4/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.6/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.9/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.9/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.9/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 24.9/30.3 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 25.2/30.3 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 25.4/30.3 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 25.7/30.3 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 25.7/30.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 26.0/30.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 26.2/30.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 26.2/30.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 26.5/30.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 27.0/30.3 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 27.5/30.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 27.5/30.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 27.8/30.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 27.8/30.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 27.8/30.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 28.3/30.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 28.8/30.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 29.1/30.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  29.6/30.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: plotly, llvmlite, numba, pynndescent, hdbscan, umap-learn, bertopic\n",
      "Successfully installed bertopic-0.16.4 hdbscan-0.8.40 llvmlite-0.44.0 numba-0.61.0 plotly-6.0.0 pynndescent-0.5.13 umap-learn-0.5.7\n",
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keybert) (2.0.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keybert) (13.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keybert) (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keybert) (3.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (2.5.1+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: keybert\n",
      "Successfully installed keybert-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.3)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-ipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-ipy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install bertopic\n",
    "!pip install keybert\n",
    "!pip install gensim\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbbed676e2e41a9b0731b1da0f38f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15942655d2c48ccbb86950c1d5f13b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d615a4b8a6c4d14a124ac0638c59e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d193facf48eb45518af2cb987372be98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c2e732a86a4fb0b32e5114992adfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing market data: 100%|| 449/449 [28:29<00:00,  3.81s/it]    \n",
      "Processing reddit data: 100%|| 123/123 [19:54<00:00,  9.71s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete. Files saved as processed_market_data.json and processed_reddit_data.json\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove special characters and extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def get_bert_embeddings(text, tokenizer, model):\n",
    "    \"\"\"Get BERT embeddings for text\"\"\"\n",
    "    # Tokenize and get BERT embeddings\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use [CLS] token embedding as text representation\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embeddings[0]\n",
    "\n",
    "def preprocess_data_with_bert():\n",
    "    try:\n",
    "        # Load BERT tokenizer and model\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Load data\n",
    "        with open('egg_market_data.json', 'r', encoding='utf-8') as f:\n",
    "            market_data = json.load(f)\n",
    "        with open('reddit_eggs_data.json', 'r', encoding='utf-8') as f:\n",
    "            reddit_data = json.load(f)\n",
    "\n",
    "        # Process market data\n",
    "        cleaned_market_data = []\n",
    "        for post in tqdm(market_data, desc=\"Processing market data\"):\n",
    "            # Clean and combine relevant text fields\n",
    "            title = clean_text(post.get('title', ''))\n",
    "            content = clean_text(post.get('content', ''))\n",
    "            combined_text = f\"{title} {content}\"\n",
    "            \n",
    "            if combined_text.strip():  # Only process non-empty text\n",
    "                # Get BERT embeddings\n",
    "                embeddings = get_bert_embeddings(combined_text, tokenizer, model)\n",
    "                \n",
    "                # Create cleaned post object\n",
    "                cleaned_post = {\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'created': post.get('created', ''),\n",
    "                    'score': post.get('score', 0),\n",
    "                    'bert_embeddings': embeddings.tolist(),\n",
    "                    'comments': []\n",
    "                }\n",
    "                \n",
    "                # Process comments\n",
    "                for comment in post.get('comments', []):\n",
    "                    comment_text = clean_text(comment.get('text', ''))\n",
    "                    if comment_text:\n",
    "                        comment_embeddings = get_bert_embeddings(comment_text, tokenizer, model)\n",
    "                        cleaned_post['comments'].append({\n",
    "                            'text': comment_text,\n",
    "                            'score': comment.get('score', 0),\n",
    "                            'created': comment.get('created', ''),\n",
    "                            'bert_embeddings': comment_embeddings.tolist()\n",
    "                        })\n",
    "                        \n",
    "                cleaned_market_data.append(cleaned_post)\n",
    "\n",
    "        # Process reddit data similarly\n",
    "        cleaned_reddit_data = []\n",
    "        for post in tqdm(reddit_data, desc=\"Processing reddit data\"):\n",
    "            title = clean_text(post.get('title', ''))\n",
    "            body = clean_text(post.get('body', ''))\n",
    "            combined_text = f\"{title} {body}\"\n",
    "            \n",
    "            if combined_text.strip():\n",
    "                embeddings = get_bert_embeddings(combined_text, tokenizer, model)\n",
    "                \n",
    "                cleaned_post = {\n",
    "                    'title': title,\n",
    "                    'body': body,\n",
    "                    'created': post.get('created', ''),\n",
    "                    'score': post.get('score', 0),\n",
    "                    'bert_embeddings': embeddings.tolist(),\n",
    "                    'comments': []\n",
    "                }\n",
    "                \n",
    "                for comment in post.get('comments', []):\n",
    "                    comment_text = clean_text(comment.get('body', ''))\n",
    "                    if comment_text:\n",
    "                        comment_embeddings = get_bert_embeddings(comment_text, tokenizer, model)\n",
    "                        cleaned_post['comments'].append({\n",
    "                            'text': comment_text,\n",
    "                            'score': comment.get('score', 0),\n",
    "                            'created': comment.get('created', ''),\n",
    "                            'bert_embeddings': comment_embeddings.tolist()\n",
    "                        })\n",
    "                        \n",
    "                cleaned_reddit_data.append(cleaned_post)\n",
    "\n",
    "        # Save processed data\n",
    "        with open('processed_market_data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_market_data, f, indent=4, ensure_ascii=False)\n",
    "        with open('processed_reddit_data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_reddit_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(\"Data preprocessing complete. Files saved as processed_market_data.json and processed_reddit_data.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Execute the preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data_with_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install transformers gensim numpy torch tensorflow pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers gensim numpy torch tensorflow pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc7c9df49b4e56a010d4f71544ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8470b5e4ad6d4b7dba6c1413707f30fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fcde14da15403f989e9a4fb09bd5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e44dd59a7c47c390ca8fee332963eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3c6602ee7c4cbfa96d8e2aaa1041c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aabc1f73c5243d68241ab5070ed8051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c156ada1634c0a93b19567854aa048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3665d6f1a19f478689b02fa490227680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab162e69beb4e30b14ffc21d7d19201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc9b7d94e0c40e5aa1d614b562855c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394cce6e57bb4d5f815bcf7694b809d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     topic_model \u001b[38;5;241m=\u001b[39m BERTopic(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, calculate_probabilities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 43\u001b[0m     topics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic model trained successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bertopic\\_bertopic.py:493\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_representative_docs(custom_documents)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Reduce topics\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bertopic\\_bertopic.py:3990\u001b[0m, in \u001b[0;36mBERTopic._extract_topics\u001b[1;34m(self, documents, embeddings, mappings, verbose)\u001b[0m\n\u001b[0;32m   3988\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepresentation - Extracting topics from clusters using representation models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3989\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin})\n\u001b[1;32m-> 3990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_tf_idf_, words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_per_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_representations_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_words_per_topic(words, documents)\n\u001b[0;32m   3992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_topic_vectors(documents\u001b[38;5;241m=\u001b[39mdocuments, embeddings\u001b[38;5;241m=\u001b[39membeddings, mappings\u001b[38;5;241m=\u001b[39mmappings)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bertopic\\_bertopic.py:4201\u001b[0m, in \u001b[0;36mBERTopic._c_tf_idf\u001b[1;34m(self, documents_per_topic, fit, partial_fit)\u001b[0m\n\u001b[0;32m   4199\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mpartial_fit(documents)\u001b[38;5;241m.\u001b[39mupdate_bow(documents)\n\u001b[0;32m   4200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fit:\n\u001b[1;32m-> 4201\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4203\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_model\u001b[38;5;241m.\u001b[39mtransform(documents)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='conversation_starter.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    with open('processed_reddit_data.json', 'r', encoding='utf-8') as f:\n",
    "        combined_data = json.load(f)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Data file not found: {e}\")\n",
    "    raise\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(f\"JSON decode error: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Unexpected error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare documents for topic modeling\n",
    "documents = []\n",
    "for post in combined_data:\n",
    "    body = post.get('body', '')\n",
    "    if isinstance(body, str):  # Ensure the body is a string\n",
    "        documents.append(body)\n",
    "    else:\n",
    "        logging.warning(f\"Skipping non-string body: {body}\")\n",
    "\n",
    "logging.info(f\"Prepared {len(documents)} documents for topic modeling.\")\n",
    "\n",
    "# Advanced Topic Modeling with BERTopic\n",
    "try:\n",
    "    topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
    "    topics, _ = topic_model.fit_transform(documents)\n",
    "    logging.info(\"BERTopic model trained successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"BERTopic model training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Visualize Topics\n",
    "try:\n",
    "    topic_model.visualize_topics().write_html(\"topic_visualization.html\")\n",
    "    logging.info(\"Topic visualization saved to topic_visualization.html.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error visualizing topics: {e}\")\n",
    "\n",
    "# Load Gemma Model\n",
    "try:\n",
    "    model_name = \"google/gemma-2b\"  # Use a larger model if available\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    logging.info(\"Gemma model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load language model: {e}\")\n",
    "    raise\n",
    "\n",
    "def generate_conversation_starter(topic_words):\n",
    "    \"\"\"Generates a conversation starter using the Gemma model.\"\"\"\n",
    "    try:\n",
    "        prompt = f\"Generate an engaging question about eggs and market demand based on these topics: {', '.join(topic_words)}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating conversation starter: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find Egg-Related Topics and Generate Starters\n",
    "egg_related_posts = []\n",
    "for idx, post in enumerate(combined_data):\n",
    "    try:\n",
    "        # Get topic distribution for post\n",
    "        topic_dist = topic_model.get_document_info(documents[idx]).Topic\n",
    "        \n",
    "        # Check if any topics are egg-related\n",
    "        topic_words = topic_model.get_topic(topic_dist)\n",
    "        if any(word in ['egg', 'eggs', 'dha', 'chicken', 'farm'] for word, _ in topic_words):\n",
    "            # Calculate engagement score\n",
    "            engagement_score = post.get('score', 0) + sum(c.get('score', 0) for c in post.get('comments', []))\n",
    "            engagement_score *= (1 + topic_model.get_document_info(documents[idx]).Probability)\n",
    "\n",
    "            # Generate conversation starter\n",
    "            conversation_starter = generate_conversation_starter([word for word, _ in topic_words])\n",
    "            if conversation_starter:\n",
    "                egg_related_posts.append({\n",
    "                    'title': post.get('title', ''),\n",
    "                    'engagement_score': engagement_score,\n",
    "                    'topic_probability': float(topic_model.get_document_info(documents[idx]).Probability),\n",
    "                    'topic_words': [word for word, _ in topic_words],\n",
    "                    'conversation_starter': conversation_starter\n",
    "                })\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing post {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Sort by engagement score\n",
    "egg_related_posts.sort(key=lambda x: x['engagement_score'], reverse=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTop High Engagement Egg-Related Conversation Starters:\")\n",
    "for post in egg_related_posts[:5]:\n",
    "    print(f\"\\nTitle: {post['title']}\")\n",
    "    print(f\"Engagement Score: {post['engagement_score']:.2f}\")\n",
    "    print(f\"Topic Probability: {post['topic_probability']:.2f}\")\n",
    "    print(f\"Topic Words: {', '.join(post['topic_words'])}\")\n",
    "    print(f\"Conversation Starter: {post['conversation_starter']}\")\n",
    "\n",
    "# Save results\n",
    "try:\n",
    "    with open(\"egg_conversation_starters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(egg_related_posts, f, indent=4, ensure_ascii=False)\n",
    "    logging.info(\"Successfully saved conversation starters to JSON file.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving to JSON file: {e}\")\n",
    "\n",
    "# Visualize Engagement Scores\n",
    "try:\n",
    "    engagement_scores = [post['engagement_score'] for post in egg_related_posts]\n",
    "    plt.hist(engagement_scores, bins=20, edgecolor='black')\n",
    "    plt.xlabel(\"Engagement Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Engagement Scores\")\n",
    "    plt.savefig(\"engagement_scores_distribution.png\")\n",
    "    plt.close()\n",
    "    logging.info(\"Engagement scores visualization saved to engagement_scores_distribution.png.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error visualizing engagement scores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for topic modeling\n",
    "documents = [post.get('title', '') for post in combined_data if post.get('title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK stopwords (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Preprocess documents\n",
    "documents = [preprocess_text(post.get('title', '')) for post in combined_data if post.get('title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a custom CountVectorizer with stop words\n",
    "custom_stop_words = [\"the\", \"for\", \"in\", \"and\", \"is\", \"of\", \"a\", \"to\"]\n",
    "vectorizer_model = CountVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "# Initialize BERTopic with the custom vectorizer\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    vectorizer_model=vectorizer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Posts:\n",
      "\n",
      "Rank 1:\n",
      "Title: [pro/chef] macarons\n",
      "Engagement Score: 28822\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 2:\n",
      "Title: [homemade] bibimbap\n",
      "Engagement Score: 1730\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 3:\n",
      "Title: 2 dozen eggs at costco in los angeles $7.69\n",
      "Engagement Score: 1548\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 4:\n",
      "Title: sometimes you just need eggs\n",
      "Engagement Score: 1057\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 5:\n",
      "Title: a wrinkled egg from farm chickens by my house. i ate it.\n",
      "Engagement Score: 643\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 6:\n",
      "Title: as egg prices soar, trump administration plans new strategy to fight bird flu\n",
      "Engagement Score: 591\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 7:\n",
      "Title: about time we let the people in on our secret.\n",
      "Engagement Score: 545\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 8:\n",
      "Title: 12/12 eggs in my carton were double yolks\n",
      "Engagement Score: 517\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 9:\n",
      "Title: free range local farm eggs! the best i've ever tried\n",
      "Engagement Score: 481\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 10:\n",
      "Title: i'm sick of cheap customers\n",
      "Engagement Score: 420\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 11:\n",
      "Title: i'm sick of cheap customers\n",
      "Engagement Score: 418\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 12:\n",
      "Title: what is this in my egg?\n",
      "Engagement Score: 408\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 13:\n",
      "Title: as bird flu payments pass $1b and egg prices keep climbing, feds make it harder to get compensated\n",
      "Engagement Score: 389\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 14:\n",
      "Title: parting gift of mil, organic farm eggs. all four eggs had two yolks. it was pleasant surprise haha.\n",
      "Engagement Score: 376\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 15:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 335\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 16:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 334\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 17:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 333\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 18:\n",
      "Title: extremely rare finds: two packs of eggs filled with double yolk eggs! (minus 1 or 2 singles ones)\n",
      "Engagement Score: 321\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 19:\n",
      "Title: anyone know why the bird flu is making eggs so expensive, but not impacting the price of chicken meat?\n",
      "Engagement Score: 320\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 20:\n",
      "Title: picked up some farm fresh eggs, today and thought id try my hand at a shakshouka\n",
      "Engagement Score: 265\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 21:\n",
      "Title: egg prices rose 60% in 2022. one farm group claims it's a 'collusive scheme' by suppliers\n",
      "Engagement Score: 262\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 22:\n",
      "Title: egg prices rose 60% in 2022. one farm group claims it's a 'collusive scheme' by suppliers\n",
      "Engagement Score: 259\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 23:\n",
      "Title: dozen egg salad sandwiches for dinner. f the egg shortage.\n",
      "Engagement Score: 244\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 24:\n",
      "Title: the eggs we got from our friends farm are round\n",
      "Engagement Score: 244\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 25:\n",
      "Title: check out these rainbow eggs i got from the market.\n",
      "Engagement Score: 214\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 26:\n",
      "Title: $54 in eggs from a couple of local backyard producers i help keep in business.\n",
      "Engagement Score: 205\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 27:\n",
      "Title: the price for 18 eggs in california\n",
      "Engagement Score: 202\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 28:\n",
      "Title: egg prices in 1975!\n",
      "Engagement Score: 185\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 29:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 181\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 30:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 180\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 31:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 177\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 32:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 175\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 33:\n",
      "Title: egg i got from a farm\n",
      "Engagement Score: 169\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 34:\n",
      "Title: never seen this before. fertilized? mutation? birth defect? mostly curious, eggs are from vital farms if that matters\n",
      "Engagement Score: 167\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 35:\n",
      "Title: pasture-raised eggs\n",
      "Engagement Score: 165\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 36:\n",
      "Title: how can i find out if these eggs are using artificially colored feed to make them orange?\n",
      "Engagement Score: 160\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 37:\n",
      "Title: [homemade] apple pie\n",
      "Engagement Score: 159\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 38:\n",
      "Title: farm prices\n",
      "Engagement Score: 155\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 39:\n",
      "Title: how can i find out if these eggs are using artificially colored feed to make them orange?\n",
      "Engagement Score: 154\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 40:\n",
      "Title: customer at work gave me a dozen eggs from his suburban farm. scrambled a couple for breakfast, delicious. first time having eggs that werent store bought.\n",
      "Engagement Score: 151\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 41:\n",
      "Title: not that anyone asked, i built an app that sorts by distance 7500+ farms selling direct to consumer beef, chicken, pork, milk, eggs, bison, elk, duck, turkey, and many more (usa/canada) thoughts? [meta]\n",
      "Engagement Score: 151\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 42:\n",
      "Title: need to buy the land we are renting, trying to find the best way to do it\n",
      "Engagement Score: 144\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 43:\n",
      "Title: need to buy the land we are renting, trying to find the best way to do it\n",
      "Engagement Score: 143\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 44:\n",
      "Title: overkill? nah have you seen the price of eggs??\n",
      "Engagement Score: 135\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 45:\n",
      "Title: unprofitable egg farm\n",
      "Engagement Score: 132\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 46:\n",
      "Title: usda restricts use of product of usa label to u.s.-grown meat, poultry, and eggs\n",
      "Engagement Score: 131\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 47:\n",
      "Title: unprofitable egg farm\n",
      "Engagement Score: 131\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 48:\n",
      "Title: is nellie's stepping up their egg game? i've purchased nellie's almost exclusively for a few years now and this bright orange, usually seen in farm eggs, appeared before me today. is this a sign of nutrition or enhanced feed? true sign of \"better\" eggs?\n",
      "Engagement Score: 127\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 49:\n",
      "Title: largest trade flows of agriculture products\n",
      "Engagement Score: 126\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 50:\n",
      "Title: got these beautiful blue eggs from the farm market laid by an araucana chicken. they have super orange yolks. what's your favorite way to have farm fresh eggs?\n",
      "Engagement Score: 119\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 51:\n",
      "Title: what farming project will you start with $1m?\n",
      "Engagement Score: 118\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 52:\n",
      "Title: got these beautiful blue eggs from the farm market laid by an araucana chicken. they have super orange yolks. what's your favorite way to have farm fresh eggs?\n",
      "Engagement Score: 116\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 53:\n",
      "Title: just outta the oven. 2020 winter quiche. featuring belle vie farms smoked ham, shirttail creek farms pastured eggs, french comte cheese, with fresh shallot, chives, and sauted mushrooms.\n",
      "Engagement Score: 115\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 54:\n",
      "Title: production is egg-cellent!\n",
      "Engagement Score: 114\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 55:\n",
      "Title: chickens!\n",
      "Engagement Score: 108\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 56:\n",
      "Title: saw this sub and my username demanded i join. anywho, here is a beer brat scotch egg i made a little while ago.\n",
      "Engagement Score: 99\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 57:\n",
      "Title: just saw this\n",
      "Engagement Score: 98\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 58:\n",
      "Title: we bought a dozen farm eggs from some friends and they all had this crazy hollow on the air pocket end. i've never saw it this extreme though \n",
      "Engagement Score: 96\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 59:\n",
      "Title: odds of multiple twin eggs?\n",
      "Engagement Score: 95\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 60:\n",
      "Title: eggs are consistently greyish for the last 2 or 3 months.\n",
      "Engagement Score: 93\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 61:\n",
      "Title: after cooking at home for a week, fast-food is beginning to taste like crap\n",
      "Engagement Score: 93\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 62:\n",
      "Title: funny egg keepers\n",
      "Engagement Score: 92\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 63:\n",
      "Title: edited for correction: pastured egg yolk is noticeably more orange compared to regular cage free eggs\n",
      "Engagement Score: 91\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 64:\n",
      "Title: farmers market fresh\n",
      "Engagement Score: 88\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 65:\n",
      "Title: went to make breakfast this morning, what is this?!?\n",
      "Engagement Score: 87\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 66:\n",
      "Title: egg shortage\n",
      "Engagement Score: 86\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 67:\n",
      "Title: egg shortage\n",
      "Engagement Score: 84\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 68:\n",
      "Title: egg shortage\n",
      "Engagement Score: 82\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 69:\n",
      "Title: just finished watching food inc. i thought it was utterly fascinating, what does r/food think?\n",
      "Engagement Score: 82\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 70:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 74\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 71:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 72\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 72:\n",
      "Title: hard work pays off... fought the wind and the rain and the mud... but i got an egg! *fist pump, yesss! #winning #worthit #quaileggs #farming #hardcore\n",
      "Engagement Score: 71\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 73:\n",
      "Title: gotta love happy egg heritage breed\n",
      "Engagement Score: 70\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 74:\n",
      "Title: update: eggs consistently look grey, haven't changed anything\n",
      "Engagement Score: 69\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 75:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 69\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 76:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 77:\n",
      "Title: resurrecting the farm\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 78:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 79:\n",
      "Title: 750 ($5.50)/egg. these are organic, but more importantly the first laid eggs from 12 different chickens. it was a present.\n",
      "Engagement Score: 66\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 80:\n",
      "Title: whats the healthiest brand of egg?\n",
      "Engagement Score: 65\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 81:\n",
      "Title: egg prices soared after bird flu, but federal price controls kept milk prices steady  at farmers' expense\n",
      "Engagement Score: 65\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 82:\n",
      "Title: farm fresh colors\n",
      "Engagement Score: 64\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 83:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 64\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 84:\n",
      "Title: the difference between (vital farms) pasture raised eggs and backyard chickens with conventional feed.\n",
      "Engagement Score: 63\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 85:\n",
      "Title: whats the healthiest brand of egg?\n",
      "Engagement Score: 63\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 86:\n",
      "Title: difference is sizes and color off eggs i got from a coworker\n",
      "Engagement Score: 63\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 87:\n",
      "Title: bird flu drives us egg prices to all-time highs before christmas\n",
      "Engagement Score: 62\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 88:\n",
      "Title: never in my life i thought i would see this\n",
      "Engagement Score: 60\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 89:\n",
      "Title: never in my life i thought i would see this\n",
      "Engagement Score: 60\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 90:\n",
      "Title: every time we visit the farming part of my family we get a bunch of eggs! a lot of different hens give a lot of different coloured eggs\n",
      "Engagement Score: 59\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 91:\n",
      "Title: pickled eggs from before price increases\n",
      "Engagement Score: 58\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 92:\n",
      "Title: eggs\n",
      "Engagement Score: 58\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 93:\n",
      "Title: a local farm sells eggs at a 24/7 vending machine on their driveway [oc]\n",
      "Engagement Score: 58\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 94:\n",
      "Title: cooked salted duck egg yolks\n",
      "Engagement Score: 56\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 95:\n",
      "Title: small farms, niche markets\n",
      "Engagement Score: 56\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 96:\n",
      "Title: farm confessional: i sell at a farmers market\n",
      "Engagement Score: 56\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 97:\n",
      "Title: farm confessional: i sell at a farmers market\n",
      "Engagement Score: 55\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 98:\n",
      "Title: pickled eggs from before price increases\n",
      "Engagement Score: 54\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 99:\n",
      "Title: small farms, niche markets\n",
      "Engagement Score: 54\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 100:\n",
      "Title: farmers market eggs, fruit and some cracked pepper turkey.\n",
      "Engagement Score: 52\n",
      "Sentiment: N/A\n",
      "Topic: N/A\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 100 posts saved to 'top_100_posts.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Load your JSON dataset\n",
    "with open(\"processed_market_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 2: Calculate engagement score (post score + sum of all comment scores)\n",
    "for post in data:\n",
    "    # Sum the scores of all comments\n",
    "    total_comment_score = sum(comment.get(\"score\", 0) for comment in post.get(\"comments\", []))\n",
    "    # Calculate engagement score\n",
    "    engagement_score = post.get(\"score\", 0) + total_comment_score\n",
    "    # Add the engagement score to the post\n",
    "    post[\"engagement_score\"] = engagement_score\n",
    "\n",
    "# Step 3: Sort posts by engagement score (descending)\n",
    "sorted_data = sorted(data, key=lambda x: x.get(\"engagement_score\", 0), reverse=True)\n",
    "\n",
    "# Step 4: Select top 100 posts\n",
    "top_100_posts = sorted_data[:100]\n",
    "\n",
    "# Step 5: Print top 100 posts to console\n",
    "print(\"Top 100 Posts:\")\n",
    "for i, post in enumerate(top_100_posts):\n",
    "    print(f\"\\nRank {i + 1}:\")\n",
    "    print(f\"Title: {post.get('title', 'N/A')}\")\n",
    "    print(f\"Engagement Score: {post.get('engagement_score', 0)}\")\n",
    "    print(f\"Sentiment: {post.get('sentiment', 'N/A')}\")  # Optional field\n",
    "    print(f\"Topic: {post.get('topic', 'N/A')}\")  # Optional field\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Step 6: Save top 100 posts to a JSON file\n",
    "with open(\"top_100_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(top_100_posts, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nTop 100 posts saved to 'top_100_posts.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing Batches:   0%|          | 0/150 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batches: 100%|| 150/150 [1:12:24<00:00, 28.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Posts:\n",
      "\n",
      "Rank 1:\n",
      "Title: [pro/chef] macarons\n",
      "Engagement Score: 28822\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 2:\n",
      "Title: [homemade] bibimbap\n",
      "Engagement Score: 1730\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 3:\n",
      "Title: 2 dozen eggs at costco in los angeles $7.69\n",
      "Engagement Score: 1548\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: technology\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 4:\n",
      "Title: sometimes you just need eggs\n",
      "Engagement Score: 1057\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 5:\n",
      "Title: a wrinkled egg from farm chickens by my house. i ate it.\n",
      "Engagement Score: 643\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 6:\n",
      "Title: as egg prices soar, trump administration plans new strategy to fight bird flu\n",
      "Engagement Score: 591\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 7:\n",
      "Title: about time we let the people in on our secret.\n",
      "Engagement Score: 545\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 8:\n",
      "Title: 12/12 eggs in my carton were double yolks\n",
      "Engagement Score: 517\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 9:\n",
      "Title: free range local farm eggs! the best i've ever tried\n",
      "Engagement Score: 481\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 10:\n",
      "Title: i'm sick of cheap customers\n",
      "Engagement Score: 420\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 11:\n",
      "Title: i'm sick of cheap customers\n",
      "Engagement Score: 418\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 12:\n",
      "Title: what is this in my egg?\n",
      "Engagement Score: 408\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 13:\n",
      "Title: as bird flu payments pass $1b and egg prices keep climbing, feds make it harder to get compensated\n",
      "Engagement Score: 389\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 14:\n",
      "Title: parting gift of mil, organic farm eggs. all four eggs had two yolks. it was pleasant surprise haha.\n",
      "Engagement Score: 376\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 15:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 335\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 16:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 334\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 17:\n",
      "Title: where food really comes from\n",
      "Engagement Score: 333\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 18:\n",
      "Title: extremely rare finds: two packs of eggs filled with double yolk eggs! (minus 1 or 2 singles ones)\n",
      "Engagement Score: 321\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 19:\n",
      "Title: anyone know why the bird flu is making eggs so expensive, but not impacting the price of chicken meat?\n",
      "Engagement Score: 320\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 20:\n",
      "Title: picked up some farm fresh eggs, today and thought id try my hand at a shakshouka\n",
      "Engagement Score: 265\n",
      "Sentiment: LABEL_0 (Score: 0.51)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 21:\n",
      "Title: egg prices rose 60% in 2022. one farm group claims it's a 'collusive scheme' by suppliers\n",
      "Engagement Score: 262\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 22:\n",
      "Title: egg prices rose 60% in 2022. one farm group claims it's a 'collusive scheme' by suppliers\n",
      "Engagement Score: 259\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 23:\n",
      "Title: dozen egg salad sandwiches for dinner. f the egg shortage.\n",
      "Engagement Score: 244\n",
      "Sentiment: LABEL_0 (Score: 0.55)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 24:\n",
      "Title: the eggs we got from our friends farm are round\n",
      "Engagement Score: 244\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 25:\n",
      "Title: check out these rainbow eggs i got from the market.\n",
      "Engagement Score: 214\n",
      "Sentiment: LABEL_0 (Score: 0.51)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 26:\n",
      "Title: $54 in eggs from a couple of local backyard producers i help keep in business.\n",
      "Engagement Score: 205\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 27:\n",
      "Title: the price for 18 eggs in california\n",
      "Engagement Score: 202\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 28:\n",
      "Title: egg prices in 1975!\n",
      "Engagement Score: 185\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 29:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 181\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 30:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 180\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 31:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 177\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 32:\n",
      "Title: how can i profit farming 200 acres with no experience?\n",
      "Engagement Score: 175\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 33:\n",
      "Title: egg i got from a farm\n",
      "Engagement Score: 169\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 34:\n",
      "Title: never seen this before. fertilized? mutation? birth defect? mostly curious, eggs are from vital farms if that matters\n",
      "Engagement Score: 167\n",
      "Sentiment: LABEL_0 (Score: 0.51)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 35:\n",
      "Title: pasture-raised eggs\n",
      "Engagement Score: 165\n",
      "Sentiment: LABEL_1 (Score: 0.50)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 36:\n",
      "Title: how can i find out if these eggs are using artificially colored feed to make them orange?\n",
      "Engagement Score: 160\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 37:\n",
      "Title: [homemade] apple pie\n",
      "Engagement Score: 159\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 38:\n",
      "Title: farm prices\n",
      "Engagement Score: 155\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 39:\n",
      "Title: how can i find out if these eggs are using artificially colored feed to make them orange?\n",
      "Engagement Score: 154\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 40:\n",
      "Title: customer at work gave me a dozen eggs from his suburban farm. scrambled a couple for breakfast, delicious. first time having eggs that werent store bought.\n",
      "Engagement Score: 151\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 41:\n",
      "Title: not that anyone asked, i built an app that sorts by distance 7500+ farms selling direct to consumer beef, chicken, pork, milk, eggs, bison, elk, duck, turkey, and many more (usa/canada) thoughts? [meta]\n",
      "Engagement Score: 151\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: technology\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 42:\n",
      "Title: need to buy the land we are renting, trying to find the best way to do it\n",
      "Engagement Score: 144\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 43:\n",
      "Title: need to buy the land we are renting, trying to find the best way to do it\n",
      "Engagement Score: 143\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 44:\n",
      "Title: overkill? nah have you seen the price of eggs??\n",
      "Engagement Score: 135\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 45:\n",
      "Title: unprofitable egg farm\n",
      "Engagement Score: 132\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 46:\n",
      "Title: usda restricts use of product of usa label to u.s.-grown meat, poultry, and eggs\n",
      "Engagement Score: 131\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: politics\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 47:\n",
      "Title: unprofitable egg farm\n",
      "Engagement Score: 131\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 48:\n",
      "Title: is nellie's stepping up their egg game? i've purchased nellie's almost exclusively for a few years now and this bright orange, usually seen in farm eggs, appeared before me today. is this a sign of nutrition or enhanced feed? true sign of \"better\" eggs?\n",
      "Engagement Score: 127\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 49:\n",
      "Title: largest trade flows of agriculture products\n",
      "Engagement Score: 126\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 50:\n",
      "Title: got these beautiful blue eggs from the farm market laid by an araucana chicken. they have super orange yolks. what's your favorite way to have farm fresh eggs?\n",
      "Engagement Score: 119\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 51:\n",
      "Title: what farming project will you start with $1m?\n",
      "Engagement Score: 118\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 52:\n",
      "Title: got these beautiful blue eggs from the farm market laid by an araucana chicken. they have super orange yolks. what's your favorite way to have farm fresh eggs?\n",
      "Engagement Score: 116\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 53:\n",
      "Title: just outta the oven. 2020 winter quiche. featuring belle vie farms smoked ham, shirttail creek farms pastured eggs, french comte cheese, with fresh shallot, chives, and sauted mushrooms.\n",
      "Engagement Score: 115\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 54:\n",
      "Title: production is egg-cellent!\n",
      "Engagement Score: 114\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 55:\n",
      "Title: chickens!\n",
      "Engagement Score: 108\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 56:\n",
      "Title: saw this sub and my username demanded i join. anywho, here is a beer brat scotch egg i made a little while ago.\n",
      "Engagement Score: 99\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 57:\n",
      "Title: just saw this\n",
      "Engagement Score: 98\n",
      "Sentiment: LABEL_0 (Score: 0.51)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 58:\n",
      "Title: we bought a dozen farm eggs from some friends and they all had this crazy hollow on the air pocket end. i've never saw it this extreme though \n",
      "Engagement Score: 96\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 59:\n",
      "Title: odds of multiple twin eggs?\n",
      "Engagement Score: 95\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 60:\n",
      "Title: eggs are consistently greyish for the last 2 or 3 months.\n",
      "Engagement Score: 93\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: technology\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 61:\n",
      "Title: after cooking at home for a week, fast-food is beginning to taste like crap\n",
      "Engagement Score: 93\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 62:\n",
      "Title: funny egg keepers\n",
      "Engagement Score: 92\n",
      "Sentiment: LABEL_0 (Score: 0.51)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 63:\n",
      "Title: edited for correction: pastured egg yolk is noticeably more orange compared to regular cage free eggs\n",
      "Engagement Score: 91\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: technology\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 64:\n",
      "Title: farmers market fresh\n",
      "Engagement Score: 88\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 65:\n",
      "Title: went to make breakfast this morning, what is this?!?\n",
      "Engagement Score: 87\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 66:\n",
      "Title: egg shortage\n",
      "Engagement Score: 86\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 67:\n",
      "Title: egg shortage\n",
      "Engagement Score: 84\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 68:\n",
      "Title: egg shortage\n",
      "Engagement Score: 82\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 69:\n",
      "Title: just finished watching food inc. i thought it was utterly fascinating, what does r/food think?\n",
      "Engagement Score: 82\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 70:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 74\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 71:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 72\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 72:\n",
      "Title: hard work pays off... fought the wind and the rain and the mud... but i got an egg! *fist pump, yesss! #winning #worthit #quaileggs #farming #hardcore\n",
      "Engagement Score: 71\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 73:\n",
      "Title: gotta love happy egg heritage breed\n",
      "Engagement Score: 70\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 74:\n",
      "Title: update: eggs consistently look grey, haven't changed anything\n",
      "Engagement Score: 69\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 75:\n",
      "Title: financing a new farm from scratch?\n",
      "Engagement Score: 69\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 76:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 77:\n",
      "Title: resurrecting the farm\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 78:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 67\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 79:\n",
      "Title: 750 ($5.50)/egg. these are organic, but more importantly the first laid eggs from 12 different chickens. it was a present.\n",
      "Engagement Score: 66\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 80:\n",
      "Title: whats the healthiest brand of egg?\n",
      "Engagement Score: 65\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 81:\n",
      "Title: egg prices soared after bird flu, but federal price controls kept milk prices steady  at farmers' expense\n",
      "Engagement Score: 65\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 82:\n",
      "Title: farm fresh colors\n",
      "Engagement Score: 64\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 83:\n",
      "Title: i want to talk to a poultry farmer.\n",
      "Engagement Score: 64\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 84:\n",
      "Title: the difference between (vital farms) pasture raised eggs and backyard chickens with conventional feed.\n",
      "Engagement Score: 63\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 85:\n",
      "Title: whats the healthiest brand of egg?\n",
      "Engagement Score: 63\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 86:\n",
      "Title: difference is sizes and color off eggs i got from a coworker\n",
      "Engagement Score: 63\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 87:\n",
      "Title: bird flu drives us egg prices to all-time highs before christmas\n",
      "Engagement Score: 62\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: politics\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 88:\n",
      "Title: never in my life i thought i would see this\n",
      "Engagement Score: 60\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 89:\n",
      "Title: never in my life i thought i would see this\n",
      "Engagement Score: 60\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 90:\n",
      "Title: every time we visit the farming part of my family we get a bunch of eggs! a lot of different hens give a lot of different coloured eggs\n",
      "Engagement Score: 59\n",
      "Sentiment: LABEL_0 (Score: 0.54)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 91:\n",
      "Title: pickled eggs from before price increases\n",
      "Engagement Score: 58\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 92:\n",
      "Title: eggs\n",
      "Engagement Score: 58\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 93:\n",
      "Title: a local farm sells eggs at a 24/7 vending machine on their driveway [oc]\n",
      "Engagement Score: 58\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: technology\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 94:\n",
      "Title: cooked salted duck egg yolks\n",
      "Engagement Score: 56\n",
      "Sentiment: LABEL_0 (Score: 0.53)\n",
      "Topic: entertainment\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 95:\n",
      "Title: small farms, niche markets\n",
      "Engagement Score: 56\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 96:\n",
      "Title: farm confessional: i sell at a farmers market\n",
      "Engagement Score: 56\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 97:\n",
      "Title: farm confessional: i sell at a farmers market\n",
      "Engagement Score: 55\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 98:\n",
      "Title: pickled eggs from before price increases\n",
      "Engagement Score: 54\n",
      "Sentiment: LABEL_1 (Score: 0.50)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 99:\n",
      "Title: small farms, niche markets\n",
      "Engagement Score: 54\n",
      "Sentiment: N/A (Score: 0.00)\n",
      "Topic: economy\n",
      "--------------------------------------------------\n",
      "\n",
      "Rank 100:\n",
      "Title: farmers market eggs, fruit and some cracked pepper turkey.\n",
      "Engagement Score: 52\n",
      "Sentiment: LABEL_0 (Score: 0.52)\n",
      "Topic: health\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 100 posts saved to 'top_100_posts.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm # Import tqdm for progress bar\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='sentiment_analysis.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load your JSON dataset\n",
    "try:\n",
    "    with open(\"processed_market_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"File not found: {e}\")\n",
    "    raise\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(f\"JSON decode error: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Unexpected error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize models (move outside loop for efficiency)\n",
    "try:\n",
    "    # Load CUDA devices\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # Sentiment analysis model (small and fast)\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased\", device=device)\n",
    "    # Topic classification model (specify a smaller model if possible)\n",
    "    topic_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing models: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define candidate labels for topic classification\n",
    "candidate_labels = [\"economy\", \"technology\", \"health\", \"politics\", \"entertainment\"]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 3  # Adjust based on your resources\n",
    "\n",
    "# Create batches of data to increase efficiency\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = data[i:i + batch_size]\n",
    "\n",
    "    # Extract text for batch\n",
    "    batch_text = [post.get(\"title\", \"\") + \" \".join(comment.get(\"text\", \"\") for comment in post.get(\"comments\", [])) for post in batch]\n",
    "\n",
    "    # Perform sentiment analysis for batch\n",
    "    try:\n",
    "        sentiment_results = sentiment_analyzer(batch_text)\n",
    "        for post, sentiment_result in zip(batch, sentiment_results):\n",
    "            post[\"sentiment\"] = sentiment_result['label']\n",
    "            post[\"sentiment_score\"] = sentiment_result['score']\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error performing sentiment analysis for batch: {e}\")\n",
    "        for post in batch:\n",
    "            post[\"sentiment\"] = \"N/A\"\n",
    "            post[\"sentiment_score\"] = 0.0\n",
    "\n",
    "    # Perform topic classification for batch\n",
    "    try:\n",
    "        topic_results = topic_classifier(batch_text, candidate_labels)\n",
    "        for post, topic_result in zip(batch, topic_results):\n",
    "            post[\"topic\"] = topic_result['labels'][0]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error performing topic classification for batch: {e}\")\n",
    "        for post in batch:\n",
    "            post[\"topic\"] = \"N/A\"\n",
    "\n",
    "# Calculate engagement score (post score + sum of all comment scores)\n",
    "for post in data:\n",
    "    # Sum the scores of all comments\n",
    "    total_comment_score = sum(comment.get(\"score\", 0) for comment in post.get(\"comments\", []))\n",
    "    # Calculate engagement score\n",
    "    engagement_score = post.get(\"score\", 0) + total_comment_score\n",
    "    # Add the engagement score to the post\n",
    "    post[\"engagement_score\"] = engagement_score\n",
    "\n",
    "# Sort posts by engagement score (descending)\n",
    "sorted_data = sorted(data, key=lambda x: x.get(\"engagement_score\", 0), reverse=True)\n",
    "\n",
    "# Select top 100 posts\n",
    "top_100_posts = sorted_data[:100]\n",
    "\n",
    "# Print top 100 posts to console\n",
    "print(\"Top 100 Posts:\")\n",
    "for i, post in enumerate(top_100_posts):\n",
    "    print(f\"\\nRank {i + 1}:\")\n",
    "    print(f\"Title: {post.get('title', 'N/A')}\")\n",
    "    print(f\"Engagement Score: {post.get('engagement_score', 0)}\")\n",
    "    print(f\"Sentiment: {post.get('sentiment', 'N/A')} (Score: {post.get('sentiment_score', 0):.2f})\")\n",
    "    print(f\"Topic: {post.get('topic', 'N/A')}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save top 100 posts to a JSON file\n",
    "try:\n",
    "    with open(\"top_100_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(top_100_posts, f, indent=4, ensure_ascii=False)\n",
    "    print(\"\\nTop 100 posts saved to 'top_100_posts.json'.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving data to JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment \n",
    "#reddit\n",
    "import praw\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='reddit_scraper.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_reddit(subreddit_name, keywords, limit=100):\n",
    "    \"\"\"\n",
    "    Scrapes Reddit for posts and comments related to the specified keywords.\n",
    "    Args:\n",
    "        subreddit_name (str): The name of the subreddit to scrape.\n",
    "        keywords (list): A list of keywords to search for.\n",
    "        limit (int): The maximum number of posts to retrieve.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing data from a relevant post.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Authenticate with Reddit API.  Replace with your credentials\n",
    "        user_agent = \"scapper 1.0 by/u/Business-Till-1699\"\n",
    "        reddit = praw.Reddit(\n",
    "            client_id='E7lnxu7KkXI9HWF0cWOFww', \n",
    "            client_secret='Ye__fRBy5a8753p62oIF9jdEt3j9Yw',\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        relevant_posts = []\n",
    "\n",
    "        for submission in subreddit.search(query=\" OR \".join(keywords), sort=\"relevance\", limit=limit):\n",
    "            post_data = {\n",
    "                'title': submission.title,\n",
    "                'url': submission.url,\n",
    "                'author': str(submission.author),\n",
    "                'subreddit': subreddit_name,\n",
    "                'upvote_ratio': submission.upvote_ratio,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'score': submission.score,\n",
    "                'created': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(submission.created)),\n",
    "                'keywords': keywords,\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            # Extract comments\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = {\n",
    "                    'author': str(comment.author),\n",
    "                    'body': comment.body,\n",
    "                    'score': comment.score,\n",
    "                    'created': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(comment.created))\n",
    "                }\n",
    "                post_data['comments'].append(comment_data)\n",
    "\n",
    "            relevant_posts.append(post_data)\n",
    "            logging.info(f\"Extracted post: {submission.title}\")\n",
    "\n",
    "        return relevant_posts\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during scraping: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    # Define subreddit and keywords\n",
    "    subreddit_name = \"eggs\"  # Adjust as needed\n",
    "    keywords = [\n",
    "    \"organic_eggs\", \"pasture_raised\", \"cage_free\", \"sustainable_poultry\", \"farm_fresh_eggs\",\n",
    "    \"organic_sweetener\", \"natural_sugar\", \"vegan_friendly\", \"farm_to_table\", \"locally_sourced\"\n",
    "    ]\n",
    "    limit = 100  # Number of posts to extract\n",
    "\n",
    "    # Scrape Reddit\n",
    "    reddit_data = scrape_reddit(subreddit_name, keywords, limit)\n",
    "\n",
    "    # Save data to JSON file\n",
    "    try:\n",
    "        with open(\"reddit_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(reddit_data, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Successfully saved data to reddit_data.json\")\n",
    "        logging.info(\"Successfully saved data to reddit_eggs_data.json\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reddit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2024.8.30)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data for keyword 'farmmade' in r/eggs\n",
      "Collected data for keyword 'farm made eggs' in r/eggs\n",
      "Collected data for keyword 'farm-made' in r/eggs\n",
      "Collected data for keyword 'free-range eggs' in r/eggs\n",
      "Collected data for keyword 'Free Range Eggs' in r/eggs\n",
      "Collected data for keyword 'Farmmade' in r/eggs\n",
      "Collected data for keyword 'Farm Made' in r/eggs\n",
      "Collected data for keyword 'eggs' in r/eggs\n",
      "Error collecting data from r/farm made eggs for 'farmmade': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'farm made eggs': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'farm-made': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'free-range eggs': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'Free Range Eggs': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'Farmmade': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'Farm Made': received 404 HTTP response\n",
      "Error collecting data from r/farm made eggs for 'eggs': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'farmmade': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'farm made eggs': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'farm-made': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'free-range eggs': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'Free Range Eggs': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'Farmmade': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'Farm Made': received 404 HTTP response\n",
      "Error collecting data from r/free range eggs for 'eggs': received 404 HTTP response\n",
      "\n",
      "Collected 427 relevant posts\n",
      "Market research data saved to reddit_data.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Reddit API connection\n",
    "user_agent = \"market_research_bot 1.0\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id='E7lnxu7KkXI9HWF0cWOFww',\n",
    "    client_secret='Ye__fRBy5a8753p62oIF9jdEt3j9Yw', \n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define search parameters\n",
    "subreddits = [\"eggs\",\"farm made eggs\",\"free range eggs\"]\n",
    "keywords = [\n",
    "    \"farmmade\", \"farm made eggs\", \"farm-made\", \"free-range eggs\", \"Free Range Eggs\",\n",
    "    \"Farmmade\", \"Farm Made\",\"eggs\"\n",
    "]\n",
    "limit = 200  # Posts per keyword\n",
    "\n",
    "market_data = []\n",
    "\n",
    "# Function to calculate engagement score\n",
    "def calculate_engagement(score, upvote_ratio, num_comments):\n",
    "    return round((score * upvote_ratio + num_comments * 0.5) / 2, 2)\n",
    "\n",
    "# Function to get basic sentiment score\n",
    "def get_basic_sentiment(text):\n",
    "    # Simple keyword-based sentiment scoring\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best']\n",
    "    negative_words = ['bad', 'poor', 'terrible', 'worst', 'hate', 'awful']\n",
    "    \n",
    "    text = text.lower()\n",
    "    pos_count = sum(1 for word in positive_words if word in text)\n",
    "    neg_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    if pos_count == neg_count:\n",
    "        return 0.0\n",
    "    return round((pos_count - neg_count) / (pos_count + neg_count), 2)\n",
    "\n",
    "# Search across multiple subreddits\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Search for market-related posts\n",
    "            for submission in subreddit.search(keyword, limit=limit):\n",
    "                \n",
    "                # Compute sentiment score using VADER\n",
    "                sentiment = vader_analyzer.polarity_scores(submission.title + \" \" + submission.selftext)\n",
    "                sentiment_score = round(sentiment['compound'], 2)\n",
    "                \n",
    "                # Compute basic sentiment score as alternative to semantic score\n",
    "                basic_sentiment = get_basic_sentiment(submission.title + \" \" + submission.selftext)\n",
    "                \n",
    "                # Compute engagement score\n",
    "                engagement_score = calculate_engagement(submission.score, submission.upvote_ratio, submission.num_comments)\n",
    "                \n",
    "                post_data = {\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'keyword': keyword,\n",
    "                    'title': submission.title,\n",
    "                    'content': submission.selftext,\n",
    "                    'url': submission.url,\n",
    "                    'post_id': submission.id,\n",
    "                    'post_url': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'author': str(submission.author),\n",
    "                    'score': submission.score,\n",
    "                    'upvote_ratio': submission.upvote_ratio,\n",
    "                    'num_comments': submission.num_comments,\n",
    "                    'sentiment_score': sentiment_score,\n",
    "                    'basic_sentiment': basic_sentiment,\n",
    "                    'engagement_score': engagement_score,\n",
    "                    'created': time.strftime('%Y-%m-%d %H:%M:%S', \n",
    "                                          time.localtime(submission.created_utc)),\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                # Get relevant comments discussing prices, supply, demand etc.\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                for comment in submission.comments.list():\n",
    "                    if any(kw in comment.body.lower() for kw in \n",
    "                          ['price', 'cost', 'market', 'supply', 'demand', 'shortage']):\n",
    "                        \n",
    "                        # Calculate scores for comment\n",
    "                        comment_sentiment = vader_analyzer.polarity_scores(comment.body)\n",
    "                        comment_basic = get_basic_sentiment(comment.body)\n",
    "                        \n",
    "                        comment_data = {\n",
    "                            'author': str(comment.author),\n",
    "                            'text': comment.body,\n",
    "                            'score': comment.score,\n",
    "                            'comment_id': comment.id,\n",
    "                            'comment_url': f\"https://reddit.com{comment.permalink}\",\n",
    "                            'sentiment_score': round(comment_sentiment['compound'], 2),\n",
    "                            'basic_sentiment': comment_basic,\n",
    "                            'created': time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "                                                   time.localtime(comment.created_utc))\n",
    "                        }\n",
    "                        post_data['comments'].append(comment_data)\n",
    "\n",
    "                market_data.append(post_data)\n",
    "            \n",
    "            print(f\"Collected data for keyword '{keyword}' in r/{subreddit_name}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting data from r/{subreddit_name} for '{keyword}': {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save market research data\n",
    "with open('reddit_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(market_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nCollected {len(market_data)} relevant posts\")\n",
    "print(\"Market research data saved to reddit_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Reddit API credentials\n",
    "import os\n",
    "\n",
    "os.environ[\"REDDIT_CLIENT_ID\"] = \"E7lnxu7KkXI9HWF0cWOFww\"\n",
    "os.environ[\"REDDIT_CLIENT_SECRET\"] = \"Ye__fRBy5a8753p62oIF9jdEt3j9Yw\" \n",
    "os.environ[\"REDDIT_USERNAME\"] = \"Flat_Ad5698\"\n",
    "os.environ[\"REDDIT_PASSWORD\"] = \"Chandra@6176\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
